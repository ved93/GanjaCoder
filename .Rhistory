# model <- mx.model.FeedForward.create(lro, X=train.x, y=train.y,
#                                     ctx=mx.cpu(), num.round=50, array.batch.size=20,
#                                     learning.rate=2e-6, momentum=0.9, eval.metric=mx.metric.rmse)
x <-train.y
#train.y<-x
train.y<-(x-min(x))/(max(x)-min(x))
y<-test.y
test.y<-(y-min(y))/(max(y)-min(y))
p<-NULL
hidden_node_layers = NULL
s<- as.vector(neurons)
for (p in 1:layers) {
if(p<=layers )
{
p<-p+1
hidden_node_layers<-as.vector(c(hidden_node_layers, s))
s<- s+8
print(s)
#hidden_node_layers
#as.vector(hidden_node_layers)
#hidden_node_layers<-(rbind(hidden_node_layers, s))
mx.metric.rmse_one <- mx.metric.custom("rmse", function(label, pred) {
res <- sqrt(mean((label-pred)^2))
df<-(res)
return(res)
})
#t<-sink("a_file.txt")
if(yz == 1)
{
mx.set.seed(10000)
model2 <- mx.mlp(data = train.x,
label = train.y,
hidden_node = hidden_node_layers ,
out_node = 1,
dropout = c(0.5),
optimizer = "sgd",
activation =  'relu' ,
out_activation = "rmse",
learning.rate = 2e-6,
momentum = 0.9,
eval.metric = mx.metric.rmse_one,
array.layout = "rowmajor" ,
num.round=10, array.batch.size=60)
mx.model.save(model2, "model_save", iteration = 10)
ab<-colnames(Store_data)[(new_features+1):ncol(Store_data)]
ab<-paste(ab, collapse="--")
getwd()
mx.model.save(model2, paste("model", ab, sep = "") , iteration = 10)
preds = as.numeric(predict(model2, test.x , array.layout = "rowmajor"))
#as.character(ID)
ID<- as.numeric(paste(ID, collapse = ""))
#gsub("[[:space:]]", "," , as.character(ID))
Mapping_table<- data.frame(ID, modelname = paste("model", ab, sep = ""))
}
#sink()
if(xy==1)
{
ID<- as.numeric(paste(ID, collapse = ""))
Mapping_table<-sqlQuery(channel , "select * from Mapping_table")
#ID %in% unique(Mapping_table$ID)
Mapping_table$modelname<- as.character(Mapping_table$modelname)
#ID in c(6811 ,  68)
model_name<-Mapping_table[which(Mapping_table$ID %in% ID), "modelname"]
#Mapping_table[Mapping_table$ID == ID, "modelname"]
model_name<-unique(model_name)
mod2<-mx.model.load(model_name, 10)
preds = as.numeric(predict(mod2, test.x , array.layout = "rowmajor" ))
}
#capture.output(summary(model2),file="captureoutput.txt")
#out <- capture.output(summary(model2))
#model2$aux.params
#
#print(eval.metric$get)
#model2$aux.params
#model2$symbol
#model$arg.params
#graph.viz(model2$symbol$as.json())
denormalised = (preds)*(max(x)-min(x))+min(x)
res<-NULL
res <-  denormalised
res<-as.data.frame(res)
colnames(res) <- "predicted"
res$Actual <- y
res$CreateDate <- CreateDate_df$CreateDate
res$refStoreId<-test$refStoreId
res$refProductId<-test$refProductId
#res$Date<-test$Date
res$Activation_f<- m
res$Numberof_neurons<- list(hidden_node_layers)
res$Numberof_hiddenlayers<- length(hidden_node_layers)
res$RMSE <- rmse(actual = round(test.y, 3) , predicted = round(preds,3))
res$uid<-f
res$Parameters<- list(c(colnames(Store_data)[(new_features+1):ncol(Store_data)]))
#sqrt(mean((denormalised-test.y)^2))
#final_model<-rbind(as.data.frame(final_model), res)
###negetive values
if(TRUE %in% res$predicted < 0) {
res$predicted<-abs(mean(res$predicted))
}
##Outliers in Forecasted values
#str(Outliers)
if(Outliers$Outliers != "null")
{
res$Outliers_flag<-ifelse((res$predicted > min(Outliers$Outliers) || res$predicted < max(Outliers$Outliers)),"Yes","No")
}
if(Outliers$Outliers == "null")
{
res$Outliers_flag<-ifelse((res$predicted < min(temp2$SalesRevenue) || res$predicted > max(temp2$SalesRevenue)),"Yes","No")
}
#Snowfall_data$SnowFall[Snowfall_data$SnowFall < 200 ]
#sqrt(mean((denormalised-test.y)^2))
final_model<-rbind(as.data.frame(final_model), res)
final_model$CreateDate <- as.POSIXlt(final_model$CreateDate)
final_model<-final_model[order(final_model$predicted , decreasing = T), ]
}
}
if(yz==1)
{
sqlSave(channel , Mapping_table ,"Mapping_table" , append = T)
}
}
}
View(final_model)
final_model<-NULL
mx.set.seed(10000)
model2 <- mx.mlp(data = train.x,
label = train.y,
hidden_node = hidden_node_layers ,
out_node = 1,
dropout = c(0.5),
optimizer = "sgdsds",
activation =  'relu' ,
out_activation = "rmse",
learning.rate = 2e-6,
momentum = 0.9,
eval.metric = mx.metric.rmse_one,
array.layout = "rowmajor" ,
num.round=10, array.batch.size=60)
mx.set.seed(10000)
model2 <- mx.mlp(data = train.x,
label = train.y,
hidden_node = hidden_node_layers ,
out_node = 1,
dropout = c(0.5),
optimizer = "adagrad",
activation =  'relu' ,
out_activation = "rmse",
learning.rate = 2e-6,
momentum = 0.9,
eval.metric = mx.metric.rmse_one,
array.layout = "rowmajor" ,
num.round=10, array.batch.size=60)
mx.set.seed(10000)
model2 <- mx.mlp(data = train.x,
label = train.y,
hidden_node = hidden_node_layers ,
out_node = 1,
dropout = c(0.5),
optimizer = "rmsprop",
activation =  'relu' ,
out_activation = "rmse",
learning.rate = 2e-6,
momentum = 0.9,
eval.metric = mx.metric.rmse_one,
array.layout = "rowmajor" ,
num.round=10, array.batch.size=60)
mx.opt.rmsprop <- function(learning.rate=0.002,
gamma1=0.95,
gamma2=0.9,
wd=0,
rescale.grad=1,
clip_gradient = NULL,
lr_scheduler = NULL) {
# use lr as short for learing rate.
lr <- learning.rate
count       <- 0
num_update  <- 0
rmsprop <- new.env()
rmsprop$lr <- lr
rmsprop$count <- 0
rmsprop$num_update <- 0
create.state <- function(index, weight) {
return (list(n=mx.nd.zeros(dim(weight), ctx(weight)),
g=mx.nd.zeros(dim(weight), ctx(weight)),
delta=mx.nd.zeros(dim(weight), ctx(weight))))
}
update <- function(index, weight, grad, state) {
if (!is.null(lr_scheduler)){
lr_scheduler(rmsprop) ## changing lr
lr <- rmsprop$lr
## update count
indexKey <- paste0('ik', index)
if (!exists(envir = rmsprop, x = indexKey)){
assign(x = indexKey, value = 0, envir = rmsprop)
} else {
indexValue <- get(envir = rmsprop, x = indexKey)
assign(x = indexKey, value = indexValue + 1, envir = rmsprop)
rmsprop$num_update <- max(rmsprop$num_update, get(envir = rmsprop, x = indexKey))
}
}
grad <- grad * rescale.grad
if (!is.null(clip_gradient)){
if(clip_gradient >= 0){
grad_ctx <- ctx(grad)
grad <- as.array(grad)
grad <- pmax(grad, -1 * clip_gradient)
grad <- pmin(grad, clip_gradient)
grad <- mx.nd.array(grad, grad_ctx)
} else {
stop("Error: clip_gradient should be positive number.")
}
}
n <- state$n
g <- state$g
delta <- state$delta
n <- gamma1 * n + (1 - gamma1) * (grad * grad)
g <- gamma1 * g + (1 - gamma1) * grad
delta <- gamma2 * delta - lr * (grad / mx.nd.sqrt(n - g*g + 1e-4) + wd * weight)
weight <- weight + delta
state <- list(n=n, g=g, delta=delta)
return(list(weight=weight, state=state))
}
return(list(create.state=create.state, update=update))
}
mx.set.seed(10000)
model2 <- mx.mlp(data = train.x,
label = train.y,
hidden_node = hidden_node_layers ,
out_node = 1,
dropout = c(0.5),
optimizer = "rmsprop",
activation =  'relu' ,
out_activation = "rmse",
learning.rate = 2e-6,
momentum = 0.9,
eval.metric = mx.metric.rmse_one,
array.layout = "rowmajor" ,
num.round=10, array.batch.size=60)
mx.set.seed(10000)
model2 <- mx.mlp(data = train.x,
label = train.y,
hidden_node = hidden_node_layers ,
out_node = 1,
dropout = c(0.5),
optimizer = "sgd",
activation =  'relu' ,
out_activation = "rmse",
learning.rate = 2e-6,
momentum = 0.9,
eval.metric = mx.metric.rmse_one,
array.layout = "rowmajor" ,
num.round=10, array.batch.size=60)
for (i in 1:length(unique(Store_data$refStoreId))) {
SID<-unique(Store_data$refStoreId)[i]
##subset of refStoreId
SID_subset<-subset(Store_data, refStoreId==SID)
for (j in unique(SID_subset$refProductId)){
PID_subset<-subset(SID_subset, refProductId==j)
#order the dates
PID_subset<- PID_subset[order(PID_subset$CreateDate) ,]
#str(PID_subset)
#summary(PID_subset)
### ##Check the data if it is daily or weekly or monthly
if((PID_subset$CreateDate[i+1] - PID_subset$CreateDate[i]) ==1)
{
#print("Data is Daily SalesRevenue")
##I should know which var is weekly
##PID_subset[ , 6] <-
#colname<-"Rainfall"
#weekly_column<- c("Rainfall" , "Temp")
#for (colname in  )) {
#PID_subset[colname] <-1/7*(PID_subset[colname])
#}
}
if((PID_subset$CreateDate[i+1] - PID_subset$CreateDate[i]) ==7)
{
print("Data is Weekly SalesRevenue")
}
if((PID_subset$CreateDate[i+1] - PID_subset$CreateDate[i]) ==30 | (PID_subset$CreateDate[i+1] - PID_subset$CreateDate[i]) ==31)
{
print("Data is Monthly SalesRevenue")
}
####Handle missing value issue by their avg value
for (colname in 1:length(PID_subset)) {
if(TRUE %in% is.na(PID_subset[colname]))
{
PID_subset[is.na(PID_subset[colname]),colname] <-mean(PID_subset[!is.na(PID_subset[colname]),colname])
}
}
#handel outlier issue
data2 <- as.xts(PID_subset$SalesRevenue,order.by=as.POSIXct(PID_subset$CreateDate))
weekly2 <- apply.weekly(data2,colMeans)
temp2<-as.data.frame(weekly2)
temp2$CreateDate<-index(weekly2)
colnames(temp2)[1] <- "SalesRevenue"
t<-1
for (i in 1:length(temp2$CreateDate)) {
Outliers<-"null"
for (g in t:length(PID_subset$SalesRevenue)) {
if(PID_subset$SalesRevenue[g] > 1.25*temp2$SalesRevenue[i] && PID_subset$CreateDate[g] <=temp2$CreateDate[i])
{
Outliers<-as.data.frame(PID_subset$SalesRevenue[g])
colnames(Outliers) <- "Outliers"
Outliers$CreateDate<-PID_subset$CreateDate[g]
PID_subset$SalesRevenue[g] = temp2$SalesRevenue[i]
#print("This is cool")
Outliers$CreateDate <- as.POSIXlt(Outliers$CreateDate)
Outliers$UID <- f
sqlSave(channel , Outliers ,"Outliers2" , append = TRUE , rownames = F)
}
}
t<-g
}
#d <-PID_subset$SalesRevenue
#train.y<-x
#PID_subset$SalesRevenue<-(d-min(d))/(max(d)-min(d))
n<-30
#create zoo class object
x <- zoo(PID_subset$SalesRevenue)
##call the function for lag creation
z <- funcLag(x,PID_subset)
colnames(z) <- c("SalesRevenue","lag1", "lag2","lag3", "lag4","lag5", "lag6","lag7",
"lag8","lag9" ,"lag10",
"lag11","lag12", "lag13","lag14", "lag15","lag16",
"lag17","lag18","lag19", "lag20","lag21",
"lag22","lag23", "lag24","lag25", "lag26","lag27",
"lag28","lag29","lag30")
lags <- as.data.frame(z)
#Keep only complete records from data
z <- z[complete.cases(z),]
temp<-as.data.frame(z)
PID_subset <- PID_subset[31:nrow(PID_subset) ,]
features<-colnames(PID_subset)
PID_CreateDate<-PID_subset
PID_subset<-PID_subset[, c(colnames(Store_data)[(new_features+1):ncol(Store_data)])]
train.x1<-PID_subset
#temp1 <- temp[,c(1,2)]
#temp1$lag1<-NULL
#temp <- as.data.frame(temp[,1])
#train.x1 <- PID_subset[,features[5:(ncol(PID_subset)-1)]]
PID_subset <- cbind(train.x1 , temp)
##data sampling
index <- sample(1:nrow(PID_subset),round(0.9*nrow(PID_subset)))
#fh<-PID_subset[tail(PID_subset , 10),]
#train <- PID_subset[index,]
#test <- PID_subset[-index,]
#CreateDate_df <- PID_CreateDate[-index,]
num <- as.numeric(num)
print(num)
test <- tail(PID_subset , as.numeric(num) )
train  <-  tail(PID_subset , -num )
CreateDate_df <- tail(PID_CreateDate , num )
#features<-colnames(train)
#train.x1 <- train[,features]
train.y<-train[ ,"SalesRevenue"]
train[ ,"SalesRevenue"] <-NULL
train.x<-data.matrix(train[ ,])
test.y<-test[ ,"SalesRevenue"]
test[ ,"SalesRevenue"]<-NULL
test.x<-data.matrix(test[, ])
# Define the input data
#data <- mx.symbol.Variable("data")
# A fully connected hidden layer
# data: input source
# num_hidden: number of neurons in this hidden layer
#fc1 <- mx.symbol.FullyConnected(data, num_hidden=1)
# Use linear regression for the output layer
#lro <- mx.symbol.LinearRegressionOutput(fc1)
#mx.set.seed(0)
# model <- mx.model.FeedForward.create(lro, X=train.x, y=train.y,
#                                     ctx=mx.cpu(), num.round=50, array.batch.size=20,
#                                     learning.rate=2e-6, momentum=0.9, eval.metric=mx.metric.rmse)
x <-train.y
#train.y<-x
train.y<-(x-min(x))/(max(x)-min(x))
y<-test.y
test.y<-(y-min(y))/(max(y)-min(y))
p<-NULL
hidden_node_layers = NULL
s<- as.vector(neurons)
for (p in 1:layers) {
if(p<=layers )
{
p<-p+1
hidden_node_layers<-as.vector(c(hidden_node_layers, s))
s<- s+10
print(s)
#hidden_node_layers
#as.vector(hidden_node_layers)
#hidden_node_layers<-(rbind(hidden_node_layers, s))
mx.metric.rmse_one <- mx.metric.custom("rmse", function(label, pred) {
res <- sqrt(mean((label-pred)^2))
df<-(res)
return(res)
})
t<-sink("a_file.txt")
if(yz == 1)
{
mx.set.seed(10000)
model2 <- mx.mlp(data = train.x,
label = train.y,
hidden_node = hidden_node_layers ,
out_node = 1,
dropout = c(0.5),
optimizer = "sgd",
activation =  'relu' ,
out_activation = "rmse",
learning.rate = 2e-6,
momentum = 0.9,
eval.metric = mx.metric.rmse_one,
array.layout = "rowmajor" ,
num.round=10, array.batch.size=60)
mx.model.save(model2, "model_save", iteration = 10)
ab<-colnames(Store_data)[(new_features+1):ncol(Store_data)]
ab<-paste(ab, collapse="--")
getwd()
mx.model.save(model2, paste("model", ab, sep = "") , iteration = 10)
preds = as.numeric(predict(model2, test.x , array.layout = "rowmajor"))
#as.character(ID)
ID<- as.numeric(paste(ID, collapse = ""))
#gsub("[[:space:]]", "," , as.character(ID))
Mapping_table<- data.frame(ID, modelname = paste("model", ab, sep = ""))
}
sink()
if(xy==1)
{
ID<- as.numeric(paste(ID, collapse = ""))
Mapping_table<-sqlQuery(channel , "select * from Mapping_table")
#ID %in% unique(Mapping_table$ID)
Mapping_table$modelname<- as.character(Mapping_table$modelname)
#ID in c(6811 ,  68)
model_name<-Mapping_table[which(Mapping_table$ID %in% ID), "modelname"]
#Mapping_table[Mapping_table$ID == ID, "modelname"]
model_name<-unique(model_name)
mod2<-mx.model.load(model_name, 10)
preds = as.numeric(predict(mod2, test.x , array.layout = "rowmajor" ))
}
#capture.output(summary(model2),file="captureoutput.txt")
#out <- capture.output(summary(model2))
#model2$aux.params
#
#print(eval.metric$get)
#model2$aux.params
#model2$symbol
#model$arg.params
#graph.viz(model2$symbol$as.json())
denormalised = (preds)*(max(x)-min(x))+min(x)
res<-NULL
res <-  denormalised
res<-as.data.frame(res)
colnames(res) <- "predicted"
res$Actual <- y
res$CreateDate <- CreateDate_df$CreateDate
res$refStoreId<-test$refStoreId
res$refProductId<-test$refProductId
#res$Date<-test$Date
res$Activation_f<- m
res$Numberof_neurons<- list(hidden_node_layers)
res$Numberof_hiddenlayers<- length(hidden_node_layers)
res$RMSE <- rmse(actual = round(test.y, 3) , predicted = round(preds,3))
res$uid<-f
res$Parameters<- list(c(colnames(Store_data)[(new_features+1):ncol(Store_data)]))
#sqrt(mean((denormalised-test.y)^2))
#final_model<-rbind(as.data.frame(final_model), res)
###negetive values
if(TRUE %in% res$predicted < 0) {
res$predicted<-abs(mean(res$predicted))
}
##Outliers in Forecasted values
#str(Outliers)
if(Outliers$Outliers != "null")
{
res$Outliers_flag<-ifelse((res$predicted > min(Outliers$Outliers) || res$predicted < max(Outliers$Outliers)),"Yes","No")
}
if(Outliers$Outliers == "null")
{
res$Outliers_flag<-ifelse((res$predicted < min(temp2$SalesRevenue) || res$predicted > max(temp2$SalesRevenue)),"Yes","No")
}
#Snowfall_data$SnowFall[Snowfall_data$SnowFall < 200 ]
#sqrt(mean((denormalised-test.y)^2))
final_model<-rbind(as.data.frame(final_model), res)
final_model$CreateDate <- as.POSIXlt(final_model$CreateDate)
final_model<-final_model[order(final_model$predicted , decreasing = T), ]
}
}
if(yz==1)
{
sqlSave(channel , Mapping_table ,"Mapping_table" , append = T)
}
}
}
View(data2)
View(final_model)
TRUE %in% res$predicted < 0
TRUE %in% (res$predicted < 0)
res$predicted < 0
res$predicted[TRUE %in% res$predicted < 0]
res[TRUE %in% (res$predicted < 0) ,]
View(res)
res[(res$predicted < 0) ,]
res$predicted[(res$predicted < 0) ,]
res$predicted[(res$predicted < 0) ]
TRUE %in% res$predicted < 0
TRUE %in% (res$predicted < 0)
res$predicted[(res$predicted < 0) ]
res$predicted[(res$predicted < 0) ] <-abs(mean(res$predicted))
View(res)
require(mlbench)
install.packages("mlbench")
data(Sonar, package="mlbench")
View(Sonar)
?regex
